# 强化学习设计说明（改进版 v5.0）

本文档基于当前仓库代码（`formation_rl_env.py`、`dwa_controller.py`、`vehicle_model.py`、`train_ppo.py`、`config.py` 等）梳理强化学习系统的完整设计。

---

## 1. 项目目标与总体思路

### 1.1 核心目标
在直路场景（`road_half_width=2.5m`，终点 `goal_x=150m`）中，让 **4辆车** 完成：
1. **到达终点**：所有车辆安全到达目的地
2. **保持队形**：路途中尽可能保持理想编队（2×2矩形）
3. **队形恢复**：通过障碍物后自动恢复原始编队
4. **安全避障**：不与障碍物、边界、其他车辆发生碰撞

### 1.2 层级式控制架构

```
┌─────────────────────────────────────────────────────────┐
│                    上层：PPO 策略                        │
│  输入: Dict观测 {image, vector}                          │
│  输出: 编队目标点修正量 Δy (4维，时间抽象每5步决策)                     │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────┐
│                 下层：DWA 控制器 (×4)                    │
│  输入: 修正后目标点 + 障碍物                             │
│  输出: 加速度 a, 转向角 δ                                │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────┐
│                  车辆动力学：自行车模型                   │
│  状态更新: [x, y, θ, v] → 下一时刻状态                   │
└─────────────────────────────────────────────────────────┘
```

---

## 2. 模块职责与数据流

### 2.1 训练入口 (`train_ppo.py`)

- **算法**：PPO with `MultiInputPolicy`
- **特征提取**：自定义 `CombinedExtractor`
  - CNN分支：处理 4×84×84 栅格图 → 128维
  - MLP分支：处理 72维向量（3帧×24维） → 64维
  - 融合层：192维 → 256维特征
- **并行环境**：`n_envs=8`，`SubprocVecEnv`
- **关键超参数**：
  - `n_steps=2048`, `batch_size=256`, `n_epochs=10`
  - `learning_rate=1e-4`, `ent_coef=0.02`

### 2.2 环境 (`formation_rl_env.py: FormationRLEnv`)

每一步执行流程：
1. 更新虚拟领航点 `leader_x += leader_speed × dt`
2. 计算理想编队位置 `ideal_positions`
3. 自适应平滑累积：`delta = (1-α)×delta + α×action`（α=0.5窄道/0.3宽道）
4. 安全约束：确保同排目标间距≥0.64m
5. 计算实际目标点：`target = ideal + delta.reshape(4,1)`（只有dy）
6. 动态调整DWA参数（窄道：min_speed=0.4, lookahead=5.0）
7. 每车调用DWA计算控制量 `(a, δ)`
8. 自行车模型更新车辆状态
9. 计算奖励与终止条件

### 2.3 执行层

- **DWA控制器** (`dwa_controller.py`)
  - 速度空间采样 `(v, yaw_rate)`
  - 代价函数：目标朝向 + 障碍距离 + 速度偏好 + RL方向偏好
  
- **车辆模型** (`vehicle_model.py`)
  - 运动学自行车模型
  - 状态：`[x, y, θ, v]`
  - 控制：`(a, δ)`

---

## 3. 观测空间（Observation）— 改进版

**类型**：`Dict` 空间（解决POMDP问题 + 帧堆叠支持时序感知）

```python
observation_space = Dict({
    "image": Box(0, 1, shape=(4, 84, 84)),  # 栅格图
    "vector": Box(-10, 10, shape=(72,))      # 3帧×24维 状态向量
})
```

### 3.1 图像观测 (4×84×84)

| 通道 | 内容 | 值 |
|------|------|-----|
| 0 | 障碍物层 | 障碍物区域=1 |
| 1 | 道路边界层 | 越界区域=1 |
| 2 | 队友位置层 | 车辆位置=1 (3×3) |
| 3 | 理想队形层 | 理想位置=0.5 (3×3) |

- **视角原点**：编队中心偏后（前向偏置设计）
- **覆盖范围**：21m × 21m（分辨率0.25m/格）
- **前向偏置**（v4.3新增）：
  - 后方留白：5m（仅供定位）
  - 前方预瞄：16m（主要感知区域）
  - **原因**：自行车模型需要提前规划变道轨迹，后方已过信息无决策价值

### 3.2 向量观测 (72维 = 3帧 × 24维)

**帧堆叠设计**：堆叠最近3帧的状态向量，让RL感知运动趋势

每帧24维结构：

| 索引 | 内容 | 归一化 |
|------|------|--------|
| 0-3 | `delta_y` (累积修正) | 原始值 |
| 4-7 | 4车 y 位置 | ÷ road_half_width |
| 8-11 | 4车速度 v | ÷ v_max |
| 12-15 | 4车理想 y | ÷ road_half_width |
| 16-19 | 4车到障碍物距离 | ÷ 10m |
| 20-23 | 4车到障碍物方向(y) | ÷ 3m |

**总向量布局**：`[帧t-2(24维), 帧t-1(24维), 帧t(24维)]`

**设计意义**：
- `delta_accumulator` 消除POMDP问题（策略可感知累积修正量）
- **帧堆叠**使RL能感知速度/加速度趋势（如"障碍物正在逼近"）
- 车辆状态信息辅助策略决策

---

## 4. 动作空间（Action）

```python
action_space = Box(low=-1.0, high=1.0, shape=(4,))  # 只有dy（dx无效已移除）
```

- **语义**：4车横向修正量 `[dy0, dy1, dy2, dy3]`（范围[-1,1]米）
- **自适应平滑累积**（v5.0更新）：
  ```python
  # 窄道时α=0.5响应更快，宽道时α=0.3更平滑
  adaptive_alpha = 0.5 if passage_width < 2.1 else 0.3
  delta_accumulator = (1 - adaptive_alpha) * delta_accumulator + adaptive_alpha * last_action
  delta_accumulator = clip(delta_accumulator, -2.0, 2.0)
  ```
- **目标位置安全约束**（v5.0新增）：同排两车目标y间距强制>=0.64m
- **实际目标**：`target_positions = ideal_positions + delta_accumulator.reshape(4, 2)`

---

## 5. 奖励函数（Reward）— 重新设计

### 5.1 设计原则
1. **目标导向**：到达终点是首要目标
2. **安全第一**：碰撞给予强烈惩罚
3. **队形自适应**：有障碍时允许变形，无障碍时恢复队形
4. **连续反馈**：距离场提供连续的安全信号
5. **平滑过渡**：避免if/else硬切换，使用连续alpha因子
6. **梯度平衡**：指数惩罚确保最后一刻斥力>引力

### 5.2 奖励组成

```python
def _compute_reward(self, target_positions):  # v5.0: 接收安全修正后的目标
    reward = 0.0
    
    # ========== 1. 碰撞惩罚（终止性） ========== 
    if collision:
        reward -= 500.0
        return reward
    
    # ========== 2. 距离场惩罚（指数形式） ========== 
    # 障碍物接近惩罚
    if min_obstacle_dist < 0.5:
        reward -= 3.0 * exp(2.0 * (0.5 - min_obstacle_dist))
    elif min_obstacle_dist < 5.0:
        reward -= 2.0 * (1.0 - min_obstacle_dist / 5.0) ** 2
    
    # 边界接近惩罚
    if min_boundary_dist < 0.3:
        reward -= 3.0 * exp(2.0 * (0.3 - min_boundary_dist))
    elif min_boundary_dist < 2.0:
        reward -= 2.0 * (1.0 - min_boundary_dist / 2.0) ** 2
    
    # 车车接近惩罚（v5.0新增）
    if min_car_dist < 0.64:  # 安全距离
        reward -= 5.0 * exp(3.0 * (0.64 - min_car_dist))
    elif min_car_dist < 1.5:
        reward -= 2.0 * (1.0 - (min_car_dist - 0.64) / 0.86) ** 2
    
    # ========== 3. 前进奖励 ========== 
    reward += 12.0 * progress
    reward -= 0.05 * max(0, avg_x - min_x - 3.5)  # 掉队惩罚
    
    # ========== 4. 队形误差惩罚 ========== 
    alpha = 1.0 - clip(min_obstacle_dist / 15.0, 0.0, 1.0)  # 0(安全)~1(危险)
    
    w_shape = 0.2
    w_scale = 0.1 * (1.0 - alpha) + 0.05 * alpha
    w_pos = 0.5 * (1.0 - alpha) + 0.3 * alpha  # v5.0: 危险时保持0.3（原为0）
    
    reward -= (w_shape * shape_error + w_scale * scale_error + w_pos * position_error)
    
    # ========== 5. 队形恢复奖励 ========== 
    if alpha < 0.2 and sqrt(position_error) < 0.3:
        reward += 1.0
    
    # ========== 6. 到达终点奖励 ========== 
    if all(car.x >= goal_x):
        reward += 500.0
        if sqrt(position_error) < 0.5:
            reward += 100.0
    
    return reward
```

### 5.3 奖励权重总结（v4.4更新）

| 组件 | 值 | 触发条件 |
|------|-----|----------|
| 碰撞惩罚 | **-500** | 发生碰撞 |
| 障碍物接近(极近) | 3×exp() | 距离 < 0.5m |
| 障碍物接近(一般) | 2×()² | 距离 < 5m |
| 边界接近(极近) | 3×exp() | 距离 < 0.3m |
| 边界接近(一般) | 2×()² | 距离 < 2m |
| 前进奖励 | **+12/m** | 每米前进 |
| 掉队惩罚 | **-0.05×** | 最慢车落后>3.5m |
| 形状畸变惩罚 | **-0.2×shape** | 始终（允许变形） |
| 尺度变化惩罚(安全) | **-0.1×scale** | alpha≈0 |
| 尺度变化惩罚(危险) | **-0.05×scale** | alpha≈1 |
| 位置偏离惩罚(安全) | **-0.5×pos** | alpha≈0 |
| 位置偏离惩罚(危险) | **-0.3×pos** | alpha≈1（v5.0修正） |
| 车车接近(极近) | **5×exp()** | 距离 < 0.64m |
| 车车接近(一般) | **2×()²** | 距离 < 1.5m |
| 队形恢复奖励 | +1 | 无障碍且位置误差<0.3m |
| 到达终点 | +500 | 所有车到达 |
| 完美队形奖励 | +100 | 终点时位置误差<0.5m |

### 5.4 形状误差计算（几何约束）

```python
def _compute_shape_error(positions):
    """
    基于矩形几何性质：
    1. 对边平行：v01 ∥ v23, v02 ∥ v13
    2. 邻边垂直：v01 ⊥ v02
    """
    # 叉积衡量平行度（=0表示平行）
    parallel_error = |cross(v01, v23)| / (|v01|×|v23|)
    
    # 点积衡量垂直度（=0表示垂直）
    perpendicular_error = |dot(v01, v02)| / (|v01|×|v02|)
    
    return parallel_error + perpendicular_error
```

**设计意义**：窄门场景(S3)需要缩小队形，此设计允许"等比缩放"但惩罚"歪扭变形"

---

## 6. 终止条件（Done）

```python
terminated = True if:
    - 任一车辆与障碍物碰撞
    - 任一车辆超出道路边界
    - 任意两车碰撞（距离 < 2×car_radius）
    - 所有车辆到达终点（x >= goal_x）

truncated = True if:
    - step_count >= max_steps (2000)
```

---

## 7. 训练配置

### 7.1 网络结构

```python
CombinedExtractor:
├── CNN分支 (图像 4×84×84)
│   ├── Conv2d(4→32, k=8, s=4) + ReLU
│   ├── Conv2d(32→64, k=4, s=2) + ReLU
│   ├── Conv2d(64→64, k=3, s=1) + ReLU
│   ├── Flatten
│   └── Linear(→128) + ReLU
│
├── MLP分支 (向量 72维 = 3帧×24维)
│   ├── Linear(72→64) + ReLU
│   └── Linear(64→64) + ReLU
│
└── 融合层
    └── Linear(192→256) + ReLU

Policy/Value Networks:
├── pi: [256, 128] → 4 (动作，只有dy)
└── vf: [256, 128] → 1 (价值)
```

### 7.2 训练超参数

| 参数 | 值 | 说明 |
|------|-----|------|
| n_envs | **8** | 并行环境数（v5.0调整） |
| n_steps | 2048 | Rollout长度 |
| batch_size | 256 | 批大小 |
| n_epochs | 10 | 每次更新epochs |
| learning_rate | 1e-4 | 初始学习率（微调时3e-5） |
| gamma | 0.99 | 折扣因子 |
| gae_lambda | 0.95 | GAE参数 |
| clip_range | 0.2 | PPO裁剪范围 |
| ent_coef | 0.02 | 熵系数 |

---

## 8. 障碍物场景

定义在 `config.py: OBSTACLE_SCENARIOS`：

| 场景 | 描述 | 通过策略 |
|------|------|----------|
| main | 综合5种障碍 | 完整测试 |
| s1_right | 右侧障碍 (25m) | 左移通过 |
| s2_left | 左侧障碍 (25m) | 右移通过 |
| s3_narrow | 两侧障碍 (25m) | 收缩队形通过窄道 |
| s4_center_small | 中间小障碍 | 保持队形绕行 |
| s5_center_large | 中间大障碍 | 分两边通过 |
| s6_very_narrow | 极窄通道 (~1m) | 一字长蛇阵纵队通过 |

### 8.2 课程学习顺序（v5.0更新）

| 阶段 | 场景 | 训练步数 | 说明 |
|------|------|----------|------|
| 1 | s4_center_small | 50万 | 入门：小障碍物 |
| 2 | s1s2_mixed | 80万 | 泛化：左右交替障碍 |
| 3 | s5_center_large | 50万 | 分流：中间大障碍（不需收缩） |
| 4 | s3_narrow | 100万 | 窄道：收缩队形通过 |
| 5 | s6_very_narrow | 150万 | 极窄：一字纵队 |

**调整原因**：S5分流不需要收缩队形，难度低于S3窄道，应先学习。

---

## 9. 设计改进总结

| 问题 | 原设计 | 改进方案 | 状态 |
|------|--------|----------|------|
| POMDP问题 | 累积器未入观测 | Dict观测 + 向量特征 | ✅ 已完成 |
| 奖励目标错误 | 惩罚修正量大小 | 惩罚真实队形误差 | ✅ 已完成 |
| 碰撞惩罚稀疏 | 只有-100终止惩罚 | 距离场连续惩罚 | ✅ 已完成 |
| 队形恢复缺失 | 无恢复机制 | 检测安全时奖励恢复 | ✅ 已完成 |
| 掉队无惩罚 | 只看avg_x | 增加min_x掉队惩罚 | ✅ 已完成 |
| 纯图像观测 | 缺少状态信息 | 图像+向量混合观测 | ✅ 已完成 |
| **时序感知缺失** | 单帧静态观测 | **帧堆叠(3帧×24维=72维)** | ✅ 已完成 |
| **队形缩放被惩罚** | 惩罚所有队形偏离 | **形状/尺度分离** | ✅ 已完成 |
| **障碍物信息缺失** | 仅图像观测障碍物 | **向量添加距离+方向** | ✅ 已完成 |
| **动作范围太小** | [-0.2, 0.2] | **扩大到[-1.0, 1.0]** | ✅ 已完成 |
| **proximity触发太晚** | safety_margin=0.56m | **扩大到3m分层预警** | ✅ 已完成 |
| **DWA动态窗口bug** | v_min>v_max无可行解 | **min_speed=0.8, max_accel=5** | ✅ 已完成 |

---

## 10. 未来改进方向

### 10.1 训练稳定性
- **VecNormalize**：添加观测/奖励归一化
- **内存优化**：降低观测分辨率或并行环境数

### 10.2 课程学习 (Curriculum Learning)
- 从简单场景(s1_right)逐步过渡到复杂场景(main)
- 动态调整障碍物难度

### 10.3 多智能体扩展
- 当前为集中式控制，可扩展为分布式MARL

---

## 11. 使用说明

### 训练
```bash
python train_ppo.py --train --timesteps 500000 --scenario main
```

### 评估
```bash
python train_ppo.py --eval models/xxx/best_model.zip
```

### 可视化测试
```bash
python test_model_visual.py models/xxx/best_model.zip s1_right
```

---

*文档版本: v4.2 | 最后更新: 2026-01-18*

---

## 13. v4.2 Action空间简化

### 13.1 问题发现

**原设计**：8维action `[dx0,dy0,dx1,dy1,dx2,dy2,dx3,dy3]`

**问题**：DWA目标x固定为`car.x + lookahead`，dx修正被忽略，4/8维动作无效。

### 13.2 修复方案

- Action: 8维 → **4维** `[dy0,dy1,dy2,dy3]`
- Vector: 28维/帧 → **24维/帧**（delta从8维减为4维）
- 总向量: 84维 → **72维**

### 13.3 改进效果

- 减少无效动作维度，提高学习效率
- 减少探索空间，加快收敛
- 降低网络复杂度

---

## 12. v4.0 关键修复记录

### 12.1 DWA动态窗口bug（根本原因）

**问题**：当`min_speed=0.1`但车辆初始速度较低时，动态窗口计算导致`v_min > v_max`，无可行轨迹，DWA调用fallback_control使车辆停止。

**修复**：
- `min_speed`: 0.1 → **0.8** （强制保持前进）
- `max_accel`: 2.0 → **5.0** （确保动态窗口有效）

### 12.2 向量观测缺少障碍物信息

**问题**：RL只能从84x84图像中提取障碍物信息，学习难度大。

**修复**：向量观测增加8维（每车到最近前方障碍物的距离和y方向），28维×3帧=84维。

### 12.3 动作范围太小

**问题**：action范围[-0.2, 0.2]，累积机制下无法快速响应避障需求。

**修复**：扩大到[-1.0, 1.0]，配合smooth_factor=0.3实现平滑且快速的响应。

### 12.4 proximity检测范围太小

**问题**：原设计只在`safety_margin=0.56m`内触发惩罚，RL几乎没有学习时间。

**修复**：分层预警设计，障碍物3m开始预警、1m危险区；边界1.5m预警、0.5m危险区。

---

## 13. v4.4 更新（关键修复）

### 13.1 时间抽象（Temporal Abstraction）
- **问题**：RL每步(0.1秒)决策导致微操过头、短视、训练慢
- **修复**：`decision_interval=5`，RL每5步(0.5秒)决策一次，中间保持上次action
- **效果**：RL是"舰长"定航向，DWA是"舵手"执行

### 13.2 前向偏置观测窗口
- **问题**：以编队中心为原点，后方50%像素是废数据
- **修复**：`center_x += 5.0`，前移5m，前方可看16m，后方仅5m
- **效果**：自行车模型有更长反应时间规划变道

### 13.3 DWA队友障碍物处理（严重BUG修复）
- **问题**：前方队友被当作障碍物，后车不敢加速追赶，gap扩大到59m
- **修复**：`is_front_teammate = dx > 1.0`，前方队友不作为障碍物
- **效果**：gap从59m降到2m，scale_error从28.56降到0.02，Reward从963升到4106

### 13.4 奖励权重调整
- `lagging_penalty`：0.3 → 0.05
- `w_shape`：2.0 → 0.2
- `w_scale`：1.0 → 0.1

---

## 14. v4.5 更新（预训练与课程学习优化）

### 14.1 监督学习预训练（Behavior Cloning）
- **目的**：为RL训练提供良好初始策略
- **方案**：empty场景下用零action训练，让模型学会"保持编队"
- **脚本**：`pretrain_bc.py`
- **效果**：预训练模型3/3到达目标，reward=3643

### 14.2 课程学习调整
- **变更**：移除empty场景（改为单独预训练）
- **流程**：
  1. `python pretrain_bc.py` - 监督学习预训练
  2. `python curriculum_train.py` - 课程学习RL训练（从pretrain_bc开始）
- **默认参数**：n_envs=4（减少内存占用）

### 14.3 DWA横向响应优化
- **问题**：theta≈0时sin(theta)≈0，横向移动极慢
- **修复**：`rl_direction_weight: 3.0 → 5.0`，增强横向追踪权重
- **效果**：DWA更积极响应RL的横向指令

### 14.4 初始位置固定
- **变更**：移除随机初始位置扰动
- **理由**：预训练为零action，不需要学习位置修正
- **效果**：训练更稳定，结果可复现

### 14.5 TensorBoard奖励分量记录
- **新增**：TrainingCallback记录各奖励分量到TensorBoard
- **分量**：progress_reward, formation_penalty, lagging_penalty, obstacle_proximity_penalty, boundary_proximity_penalty, collision
- **用途**：调试训练过程，分析奖励来源

---

## 15. v4.6 更新（代码与文档一致性修复）

### 15.1 观测向量注释修正
- **问题**：注释写"60维=3帧×20维"，实际是72维=3帧×24维
- **修复**：更新注释为"72维=3帧×24维"

### 15.2 奖励权重按v4.4更新
- **问题**：代码权重与文档不符
- **修复**：
  - `lagging_penalty`: 0.3 → **0.05**
  - `w_shape`: 2.0 → **0.2**（危险时允许变形）
  - `w_scale`: 1.0 → **0.1**

### 15.3 动作幅度修正
- **问题**：action×0.5再裁剪[-1,1]，有效偏移仅[-0.5,0.5]
- **修复**：
  - action系数：0.5 → **1.0**
  - 裁剪范围：[-1,1] → **[-2,2]**
- **效果**：横向避障能力增强，与文档描述一致

### 15.4 VecNormalize状态
- **当前状态**：**暂时移除**（奖励设计已合理，无需归一化）
- **原因**：观测已手动归一化，奖励范围可控
- **如需启用**：可在`curriculum_train.py`中添加VecNormalize包装

---

## 16. v4.7 更新（微调与动态障碍物扩展分析）

### 16.1 微调脚本 (`train_ppo.py --finetune`)

课程学习完成后，支持在完整场景(main)上进行微调：

```bash
python train_ppo.py --finetune --pretrain outputs/curriculum_xxx/stage5_xxx/best_model.zip --timesteps 500000 --scenario main
```

**微调超参数**（相比标准训练）：
| 参数 | 标准训练 | 微调 |
|------|---------|------|
| learning_rate | 1e-4 | **3e-5** |
| clip_range | 0.2 | **0.15** |
| ent_coef | 0.02 | **0.01** |
| n_envs | 16 | **8** |

### 16.2 动态障碍物扩展能力评估

#### 当前架构对动态障碍物的适配性

| 组件 | 动态适配性 | 说明 |
|------|-----------|------|
| **DWA控制器** | ✅ 天然适配 | 反应式规划，每步重新计算，自动响应位置变化 |
| **图像观测 (4帧堆叠)** | ✅ 可隐式学速度 | CNN可从连续帧推断障碍物运动趋势 |
| **向量观测** | ⚠️ 信息不足 | 仅有障碍物位置，无速度/方向信息 |
| **RL策略** | ✅ 可泛化 | MLP可学习新的避障模式 |

#### 预期表现

| 动态障碍物场景 | 预期效果 | 原因 |
|---------------|---------|------|
| 慢速移动（≤1 m/s） | ✅ 应能应对 | DWA反应式+10Hz控制频率 |
| 横向移动障碍物 | ✅ 应能应对 | 与静态避障类似 |
| 对向行驶障碍物 | ⚠️ 可能不足 | 相对速度高，反应时间短 |
| 复杂轨迹障碍物 | ❌ 需要增强 | 无预测能力 |

#### 建议的架构增强（复杂动态场景）

1. **向量观测增加速度信息**
   ```python
   # 当前: [dist_to_obs, dy_to_obs] × 4车
   # 建议: [dist_to_obs, dy_to_obs, obs_vx, obs_vy] × 4车
   ```

2. **向量观测帧堆叠**
   - 当前图像已做4帧堆叠，但向量观测是3帧
   - 可考虑统一为4帧或增加历史信息

3. **奖励函数调整**
   - 增加与动态障碍物的相对速度惩罚
   - 鼓励提前预判避让

### 16.3 训练完成状态（v5.0课程顺序）

| 阶段 | 场景 | 说明 | 状态 |
|------|------|------|------|
| 预训练 | empty | 监督学习预训练 | ⏳ 待重新训练 |
| Stage 1 | s4_center_small | 入门：小障碍物 | ⏳ 待重新训练 |
| Stage 2 | s1s2_mixed | 泛化：左右交替 | ⏳ 待重新训练 |
| Stage 3 | s5_center_large | 分流：中间大障碍 | ⏳ 待重新训练 |
| Stage 4 | s3_narrow | 窄道：收缩队形 | ⏳ 待重新训练 |
| Stage 5 | s6_very_narrow | 极窄：一字纵队 | ⏳ 待重新训练 |

**注**：v5.0修改了多处关键逻辑（自适应平滑、目标安全约束、DWA参数动态调整），需要从头重新训练。

---

## 17. v4.8 更新（极窄通道/一字长蛇阵）

### 17.1 新增场景 s6_very_narrow

**目标**：让编队学会变成一字长蛇阵（1×4纵队）通过极窄通道。

**场景设计**：
- 通道宽度：0.7m（y∈[-0.35, 0.35]），严格只能单车纵队通过
- 障碍物：两侧各2.0m高矩形，占据y∈[0.35, 2.35]和y∈[-2.35, -0.35]
- 2组障碍物（x=30m, 70m），提供多次练习机会
- 计算：车宽0.25m + 两侧各0.225m余量 = 0.7m，必须纵队通过

```python
"s6_very_narrow": [
    Obstacle(30, 1.35, 1.0, "rect", 10.0, 2.0),   # 上侧，占y=[0.35, 2.35]
    Obstacle(30, -1.35, 1.0, "rect", 10.0, 2.0),  # 下侧，占y=[-2.35, -0.35]
    Obstacle(70, 1.35, 1.0, "rect", 10.0, 2.0),
    Obstacle(70, -1.35, 1.0, "rect", 10.0, 2.0),
]
```

### 17.2 增量训练方法

基于已训练模型继续学习新阶段（无需重新训练）：

```bash
# 从Stage 6开始，使用已有模型
python curriculum_train.py --start 5 --base_model outputs/curriculum_xxx/stage5_xxx/best_model
```

**参数说明**：
- `--start 5`：从第6阶段开始（0索引，所以5=Stage 6）
- `--base_model`：指定已训练好的模型路径

### 17.3 动态Lookahead机制（关键改进）

**问题分析**：
- 当前RL只输出横向修正 `[dy0, dy1, dy2, dy3]`，无法控制纵向间距
- 如果4车同时收拢到y≈0，会在通道入口碰撞
- 需要让前车先走，后车减速等待

**解决方案**：在极窄通道模式下，根据【前方队友距离】动态调整每车的lookahead：

```python
if is_very_narrow:
    for i, car in cars:
        # 计算到前方最近队友的距离
        front_teammate_dist = min(other.x - car.x for other if other.x > car.x)
        
        # 根据前方队友距离调整lookahead
        if front_teammate_dist < 1.5:
            lookahead = 0.3   # 前方很近，几乎停止
        elif front_teammate_dist < 3.0:
            lookahead = 1.0   # 适度减速
        elif front_teammate_dist < 5.0:
            lookahead = 2.0   # 轻微减速
        else:
            lookahead = 3.0   # 正常前进（前方无队友或已走远）
        
        # 目标y强制收拢到中心线
        target_y = 0.0 + delta_y[i] * 0.3
```

**关键改进**（修复后车不动问题）：
- 不再使用固定排名，而是根据实时距离动态调整
- 当前车走远后（距离>5m），后车自动恢复正常速度
- 避免出现"永远停止"的死锁情况

**效果**：
- 最前面的车（前方无队友）正常通过
- 后面的车根据与前车距离自动调整速度
- 当前车通过后，后车自动跟上
- 4车形成纵向一字长蛇阵依次通过

### 17.4 纵队奖励机制

**核心改进**：在极窄通道区域，用"纵队奖励"替代"队形惩罚"。

```python
def _compute_column_formation_reward():
    """纵队奖励：
    1. 横向聚拢奖励：4车y坐标趋同（y_range < 0.5m时奖励最大）
    2. 纵向间距保持：避免追尾（间距1-3m最优）
    3. 中心线位置：鼓励纵队在道路中心
    """
```

### 17.5 预期行为

通过极窄通道时，编队应该：
1. 检测到极窄通道（lookahead=20m）
2. 前车继续前进，后车自动减速
3. 4车横向收拢到中心线，变成一字长蛇阵
4. 依次通过窄口（前车先过，后车跟随）
5. 通过后恢复2×2队形

---

---

## 18. v5.0 更新（窄道通过关键修复）

### 18.1 问题分析

s3_narrow场景训练失败，100%碰撞率，主要原因：

1. **DWA可行解不足**：队友避碰半径+安全间距“卡死”DWA
2. **奖励与执行不一致**：目标越界惩罚用修正前目标，DWA用修正后目标
3. **DWA高速追目标**：min_speed=0.8m/s，横向调整时间不足
4. **动作抑制**：delta_accumulator直接覆盖导致抖动
5. **RL输出可让目标交叉**：delta范围±2m可能让同排车目标位置交叉

### 18.2 修复方案

#### 优先级1：窄道时降低DWA速度+增大前视

```python
if is_narrow_passage and not is_very_narrow:
    base_lookahead = 5.0  # 3.0 -> 5.0
    for controller in self.dwa_controllers:
        controller.params.min_speed = 0.4  # 0.8 -> 0.4
```

#### 优先级2：收缩模式下同排队友不作为DWA障碍

```python
if is_contracting and same_row:
    continue  # 已有目标间距硬约束(0.64m)保护
```

#### 优先级3：奖励与执行对齐

```python
def _compute_reward(self, target_positions):  # 接收安全修正后的目标
    target_y = target_positions[i, 1]  # 而不是 ideal + delta
```

#### 优先级4：自适应平滑因子

```python
# 窄道时α=0.5响应更快，宽道时α=0.3更平滑
adaptive_alpha = 0.5 if pw < 2.1 else self.smooth_factor
delta_accumulator = (1 - adaptive_alpha) * delta_accumulator + adaptive_alpha * last_action
```

#### 目标位置安全约束

```python
# 确保同排两车目标y间距 >= 0.64m
min_safe_dist = 2 * car_radius + 0.1  # 0.64m
if abs(target_y_i - target_y_j) < min_safe_dist:
    # 调整两车目标，使间距恢复到安全距离
```

### 18.3 课程学习顺序调整

**旧顺序**：S4 → S1S2 → S3 → S5 → S6

**新顺序**：S4 → S1S2 → **S5** → **S3** → S6

**调整原因**：S5分流不需要收缩队形，难度低于S3窄道。

### 18.4 关键参数总结

| 参数 | 值 | 说明 |
|------|-----|------|
| car_radius | 0.27m | 车辆碰撞半径 |
| 碰撞阈值 | 0.54m | 2×car_radius |
| min_safe_dist | 0.64m | 目标间距安全约束 |
| scale_factor_min | 0.4 | 确保收缩后间距>=0.64m |
| teammate_radius(收缩) | 0.32m | DWA队友避碰半径 |
| min_speed(窄道) | 0.4m/s | 降低速度给横移时间 |
| lookahead(窄道) | 5.0m | 增大前视距离 |
| adaptive_alpha(窄道) | 0.5 | 快速响应 |
| adaptive_alpha(宽道) | 0.3 | 平滑响应 |

---

*文档版本: v5.0 | 最后更新: 2026-01-23*
